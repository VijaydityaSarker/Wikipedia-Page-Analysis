# Wikipedia-Page-Analysis
 Analysis of historical versions of the “Machine Learning” Wikipedia Page
• Established a data storage system for historical Wikipedia pages, automated HTML file downloads for the "Machine Learning" Wikipedia article using Bash scripting, and subsequently conducted data cleaning using a Python parsing script to extract article titles, text, references, and update dates.

• In data management, designed a tailored database based on project requirements, selected MongoDB as the data model, defined the database structure with key fields and relationships, implemented it, and loaded it with cleaned data. This database now supports MongoDB queries for data analysis, revealing distinctive trends, patterns, and emerging terms in the field of machine learning.

• Leveraged MongoDB queries to extract valuable insights from the collected data, encompassing data collection years, document counts per year, term frequencies, initial term appearances, and reference counts.
